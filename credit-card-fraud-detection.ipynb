{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-11-06T00:44:28.466086Z",
     "start_time": "2024-11-06T00:44:28.458163Z"
    }
   },
   "source": [
    "# Spark session and functions\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, isnan, when, count\n",
    "\n",
    "# Machine Learning imports\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline"
   ],
   "outputs": [],
   "execution_count": 103
  },
  {
   "cell_type": "code",
   "id": "4f285463abddc3d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T00:44:28.494345Z",
     "start_time": "2024-11-06T00:44:28.483434Z"
    }
   },
   "source": "spark = SparkSession.builder.appName('CreditCardFraudDetection').getOrCreate()",
   "outputs": [],
   "execution_count": 104
  },
  {
   "cell_type": "markdown",
   "id": "9c05cece7ea78e36",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "The dataset is available at [Kaggle](https://www.kaggle.com/mlg-ulb/creditcardfraud)."
   ]
  },
  {
   "cell_type": "code",
   "id": "6a0a8627542ecf38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T00:44:29.645011Z",
     "start_time": "2024-11-06T00:44:28.534644Z"
    }
   },
   "source": "df = spark.read.csv('dataset/creditcard.csv', header=True, inferSchema=True).cache()",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/05 19:44:29 WARN CacheManager: Asked to cache already cached data.        \n"
     ]
    }
   ],
   "execution_count": 105
  },
  {
   "cell_type": "markdown",
   "id": "7aa63ab4b9193d7f",
   "metadata": {},
   "source": [
    "## Explaining the dataset\n",
    "The dataset contains transactions made by credit cards in September 2013 by european cardholders. There are 28 features, all of them are numerical. The features V1, V2, ... V28 are the result of a PCA transformation. The only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.  \n",
    "V1, V2, ... V28 are the principal components obtained with PCA. They have been anonymized to protect sensitive information."
   ]
  },
  {
   "cell_type": "code",
   "id": "a0bf4f3ceb117e50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T00:44:29.667473Z",
     "start_time": "2024-11-06T00:44:29.662834Z"
    }
   },
   "source": [
    "df.printSchema()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Time: double (nullable = true)\n",
      " |-- V1: double (nullable = true)\n",
      " |-- V2: double (nullable = true)\n",
      " |-- V3: double (nullable = true)\n",
      " |-- V4: double (nullable = true)\n",
      " |-- V5: double (nullable = true)\n",
      " |-- V6: double (nullable = true)\n",
      " |-- V7: double (nullable = true)\n",
      " |-- V8: double (nullable = true)\n",
      " |-- V9: double (nullable = true)\n",
      " |-- V10: double (nullable = true)\n",
      " |-- V11: double (nullable = true)\n",
      " |-- V12: double (nullable = true)\n",
      " |-- V13: double (nullable = true)\n",
      " |-- V14: double (nullable = true)\n",
      " |-- V15: double (nullable = true)\n",
      " |-- V16: double (nullable = true)\n",
      " |-- V17: double (nullable = true)\n",
      " |-- V18: double (nullable = true)\n",
      " |-- V19: double (nullable = true)\n",
      " |-- V20: double (nullable = true)\n",
      " |-- V21: double (nullable = true)\n",
      " |-- V22: double (nullable = true)\n",
      " |-- V23: double (nullable = true)\n",
      " |-- V24: double (nullable = true)\n",
      " |-- V25: double (nullable = true)\n",
      " |-- V26: double (nullable = true)\n",
      " |-- V27: double (nullable = true)\n",
      " |-- V28: double (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      " |-- Class: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 106
  },
  {
   "cell_type": "markdown",
   "id": "f34b0d03e1adf2bd",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "## Checking for missing values\n",
    "First, we check for missing values in the dataset. If there are any, we will clean them up."
   ]
  },
  {
   "cell_type": "code",
   "id": "eb4438c9168c3de6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T00:44:29.851847Z",
     "start_time": "2024-11-06T00:44:29.692082Z"
    }
   },
   "source": [
    "# Checking for missing values\n",
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show(truncate=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+------+-----+\n",
      "|Time| V1| V2| V3| V4| V5| V6| V7| V8| V9|V10|V11|V12|V13|V14|V15|V16|V17|V18|V19|V20|V21|V22|V23|V24|V25|V26|V27|V28|Amount|Class|\n",
      "+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+------+-----+\n",
      "|   0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|     0|    0|\n",
      "+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 107
  },
  {
   "cell_type": "code",
   "id": "977ca2ffc58c6128",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T00:44:29.985015Z",
     "start_time": "2024-11-06T00:44:29.884252Z"
    }
   },
   "source": [
    "original_size = df.count()\n",
    "print(f'Original size: {original_size}')\n",
    "\n",
    "# Clean up missing values\n",
    "df_cleaned = df.na.drop()\n",
    "\n",
    "cleaned_size = df_cleaned.count()\n",
    "print(f'Cleaned size: {cleaned_size}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original size: 284807\n",
      "Cleaned size: 284807\n"
     ]
    }
   ],
   "execution_count": 108
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Class distribution\n",
    "We group the dataset by the 'Class' column to check the distribution of the classes."
   ],
   "id": "9f728ca8a7bddc2c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T00:44:30.136913Z",
     "start_time": "2024-11-06T00:44:30.025105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Checking for class distribution\n",
    "df_cleaned.groupBy('Class').count().show()"
   ],
   "id": "a22ae163d7d9e44e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|Class| count|\n",
      "+-----+------+\n",
      "|    1|   492|\n",
      "|    0|284315|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 109
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Assembling the features\n",
    "In PySpark MLlib, all input features should be combined into a single vector column. To do this, you use the `VectorAssembler`.  \n",
    "This is a transformer that combines a given list of columns into a single vector column. It is useful for combining raw features and features generated by different feature transformers into a single feature vector, in order to train ML models like logistic regression and decision trees.\n"
   ],
   "id": "5c16b0e1a448f807"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T00:44:30.278473Z",
     "start_time": "2024-11-06T00:44:30.168018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Select feature columns (all except 'Class', which is the label)\n",
    "feature_columns = df_cleaned.columns[:-1]  # Assuming the last column is the label\n",
    "\n",
    "# Assemble feature columns into a single vector column called \"features\"\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "df_transformed = assembler.transform(df_cleaned)\n",
    "\n",
    "# Show the transformed data\n",
    "df_transformed.select(\"features\", \"Class\").show(5, truncate=False)"
   ],
   "id": "616f52ac0c1137bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|features                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |Class|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|[0.0,-1.3598071336738,-0.0727811733098497,2.53634673796914,1.37815522427443,-0.338320769942518,0.462387777762292,0.239598554061257,0.0986979012610507,0.363786969611213,0.0907941719789316,-0.551599533260813,-0.617800855762348,-0.991389847235408,-0.311169353699879,1.46817697209427,-0.470400525259478,0.207971241929242,0.0257905801985591,0.403992960255733,0.251412098239705,-0.018306777944153,0.277837575558899,-0.110473910188767,0.0669280749146731,0.128539358273528,-0.189114843888824,0.133558376740387,-0.0210530534538215,149.62]|0    |\n",
      "|[0.0,1.19185711131486,0.26615071205963,0.16648011335321,0.448154078460911,0.0600176492822243,-0.0823608088155687,-0.0788029833323113,0.0851016549148104,-0.255425128109186,-0.166974414004614,1.61272666105479,1.06523531137287,0.48909501589608,-0.143772296441519,0.635558093258208,0.463917041022171,-0.114804663102346,-0.183361270123994,-0.145783041325259,-0.0690831352230203,-0.225775248033138,-0.638671952771851,0.101288021253234,-0.339846475529127,0.167170404418143,0.125894532368176,-0.00898309914322813,0.0147241691924927,2.69]|0    |\n",
      "|[1.0,-1.35835406159823,-1.34016307473609,1.77320934263119,0.379779593034328,-0.503198133318193,1.80049938079263,0.791460956450422,0.247675786588991,-1.51465432260583,0.207642865216696,0.624501459424895,0.066083685268831,0.717292731410831,-0.165945922763554,2.34586494901581,-2.89008319444231,1.10996937869599,-0.121359313195888,-2.26185709530414,0.524979725224404,0.247998153469754,0.771679401917229,0.909412262347719,-0.689280956490685,-0.327641833735251,-0.139096571514147,-0.0553527940384261,-0.0597518405929204,378.66]       |0    |\n",
      "|[1.0,-0.966271711572087,-0.185226008082898,1.79299333957872,-0.863291275036453,-0.0103088796030823,1.24720316752486,0.23760893977178,0.377435874652262,-1.38702406270197,-0.0549519224713749,-0.226487263835401,0.178228225877303,0.507756869957169,-0.28792374549456,-0.631418117709045,-1.0596472454325,-0.684092786345479,1.96577500349538,-1.2326219700892,-0.208037781160366,-0.108300452035545,0.00527359678253453,-0.190320518742841,-1.17557533186321,0.647376034602038,-0.221928844458407,0.0627228487293033,0.0614576285006353,123.5]  |0    |\n",
      "|[2.0,-1.15823309349523,0.877736754848451,1.548717846511,0.403033933955121,-0.407193377311653,0.0959214624684256,0.592940745385545,-0.270532677192282,0.817739308235294,0.753074431976354,-0.822842877946363,0.53819555014995,1.3458515932154,-1.11966983471731,0.175121130008994,-0.451449182813529,-0.237033239362776,-0.0381947870352842,0.803486924960175,0.408542360392758,-0.00943069713232919,0.79827849458971,-0.137458079619063,0.141266983824769,-0.206009587619756,0.502292224181569,0.219422229513348,0.215153147499206,69.99]        |0    |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 110
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Scaling the features\n",
    "Feature scaling is important because some machine learning algorithms are sensitive to the scale of input data. You can scale features using the `StandardScaler`.\n",
    "StandardScaler transforms a dataset of Vector rows, normalizing each feature to have unit standard deviation and/or zero mean. It takes parameters:\n",
    "- `inputCol`: input column name.\n",
    "- `outputCol`: output column name.\n",
    "- `withStd`: True by default. Scales the data to unit standard deviation.\n",
    "- `withMean`: False by default. Centers the data with mean before scaling. It will build a dense output, so take care when applying to sparse input."
   ],
   "id": "26b91a3b0ba4c48f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T00:44:30.945582Z",
     "start_time": "2024-11-06T00:44:30.302463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=False)\n",
    "df_scaled = scaler.fit(df_transformed).transform(df_transformed)\n",
    "\n",
    "# Summary of the scaling process\n",
    "print(f\"Mean: {scaler.explainParams()}\")\n",
    "# print(f\"Standard Deviation: {scaler.std}\")\n",
    "\n",
    "# Show the scaled data\n",
    "df_scaled.select(\"scaled_features\", \"Class\").show(5)"
   ],
   "id": "5e8dacd89c420414",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: inputCol: input column name. (current: features)\n",
      "outputCol: output column name. (default: StandardScaler_6aac85749e18__output, current: scaled_features)\n",
      "withMean: Center data with mean (default: False, current: False)\n",
      "withStd: Scale to unit standard deviation (default: True, current: True)\n",
      "+--------------------+-----+\n",
      "|     scaled_features|Class|\n",
      "+--------------------+-----+\n",
      "|[0.0,-0.694241102...|    0|\n",
      "|[0.0,0.6084952594...|    0|\n",
      "|[2.10578867609768...|    0|\n",
      "|[2.10578867609768...|    0|\n",
      "|[4.21157735219537...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 111
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T00:44:31.050954Z",
     "start_time": "2024-11-06T00:44:30.992084Z"
    }
   },
   "cell_type": "code",
   "source": "df_scaled.show(5)",
   "id": "491f4f2e395d4791",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+-------------------+----------------+------------------+-------------------+-------------------+-------------------+------------------+------------------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+--------------------+-------------------+------------------+------------------+------------------+------------------+--------------------+-------------------+------+-----+--------------------+--------------------+\n",
      "|Time|                V1|                 V2|              V3|                V4|                 V5|                 V6|                 V7|                V8|                V9|                V10|               V11|               V12|               V13|               V14|               V15|               V16|               V17|                V18|               V19|                V20|                 V21|                V22|               V23|               V24|               V25|               V26|                 V27|                V28|Amount|Class|            features|     scaled_features|\n",
      "+----+------------------+-------------------+----------------+------------------+-------------------+-------------------+-------------------+------------------+------------------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+--------------------+-------------------+------------------+------------------+------------------+------------------+--------------------+-------------------+------+-----+--------------------+--------------------+\n",
      "| 0.0|  -1.3598071336738|-0.0727811733098497|2.53634673796914|  1.37815522427443| -0.338320769942518|  0.462387777762292|  0.239598554061257|0.0986979012610507| 0.363786969611213| 0.0907941719789316|-0.551599533260813|-0.617800855762348|-0.991389847235408|-0.311169353699879|  1.46817697209427|-0.470400525259478| 0.207971241929242| 0.0257905801985591| 0.403992960255733|  0.251412098239705|  -0.018306777944153|  0.277837575558899|-0.110473910188767|0.0669280749146731| 0.128539358273528|-0.189114843888824|   0.133558376740387|-0.0210530534538215|149.62|    0|[0.0,-1.359807133...|[0.0,-0.694241102...|\n",
      "| 0.0|  1.19185711131486|   0.26615071205963|0.16648011335321| 0.448154078460911| 0.0600176492822243|-0.0823608088155687|-0.0788029833323113|0.0851016549148104|-0.255425128109186| -0.166974414004614|  1.61272666105479|  1.06523531137287|  0.48909501589608|-0.143772296441519| 0.635558093258208| 0.463917041022171|-0.114804663102346| -0.183361270123994|-0.145783041325259|-0.0690831352230203|  -0.225775248033138| -0.638671952771851| 0.101288021253234|-0.339846475529127| 0.167170404418143| 0.125894532368176|-0.00898309914322813| 0.0147241691924927|  2.69|    0|[0.0,1.1918571113...|[0.0,0.6084952594...|\n",
      "| 1.0| -1.35835406159823|  -1.34016307473609|1.77320934263119| 0.379779593034328| -0.503198133318193|   1.80049938079263|  0.791460956450422| 0.247675786588991| -1.51465432260583|  0.207642865216696| 0.624501459424895| 0.066083685268831| 0.717292731410831|-0.165945922763554|  2.34586494901581| -2.89008319444231|  1.10996937869599| -0.121359313195888| -2.26185709530414|  0.524979725224404|   0.247998153469754|  0.771679401917229| 0.909412262347719|-0.689280956490685|-0.327641833735251|-0.139096571514147| -0.0553527940384261|-0.0597518405929204|378.66|    0|[1.0,-1.358354061...|[2.10578867609768...|\n",
      "| 1.0|-0.966271711572087| -0.185226008082898|1.79299333957872|-0.863291275036453|-0.0103088796030823|   1.24720316752486|   0.23760893977178| 0.377435874652262| -1.38702406270197|-0.0549519224713749|-0.226487263835401| 0.178228225877303| 0.507756869957169| -0.28792374549456|-0.631418117709045|  -1.0596472454325|-0.684092786345479|   1.96577500349538|  -1.2326219700892| -0.208037781160366|  -0.108300452035545|0.00527359678253453|-0.190320518742841| -1.17557533186321| 0.647376034602038|-0.221928844458407|  0.0627228487293033| 0.0614576285006353| 123.5|    0|[1.0,-0.966271711...|[2.10578867609768...|\n",
      "| 2.0| -1.15823309349523|  0.877736754848451|  1.548717846511| 0.403033933955121| -0.407193377311653| 0.0959214624684256|  0.592940745385545|-0.270532677192282| 0.817739308235294|  0.753074431976354|-0.822842877946363|  0.53819555014995|   1.3458515932154| -1.11966983471731| 0.175121130008994|-0.451449182813529|-0.237033239362776|-0.0381947870352842| 0.803486924960175|  0.408542360392758|-0.00943069713232919|   0.79827849458971|-0.137458079619063| 0.141266983824769|-0.206009587619756| 0.502292224181569|   0.219422229513348|  0.215153147499206| 69.99|    0|[2.0,-1.158233093...|[4.21157735219537...|\n",
      "+----+------------------+-------------------+----------------+------------------+-------------------+-------------------+-------------------+------------------+------------------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+--------------------+-------------------+------------------+------------------+------------------+------------------+--------------------+-------------------+------+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 112
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Handling Class imbalance\n",
    "From the class distribution, we can see that the dataset is highly imbalanced. The number of non-fraudulent transactions is much higher than the number of fraudulent transactions. For our model to learn effectively, we need to balance the classes.  \n",
    "This can be done in two ways:\n",
    "1. **Undersampling**: In this method, we randomly sample the majority class to match the minority class. This can lead to loss of information.\n",
    "2. **Oversampling**: In this method, we randomly duplicate the minority class to match the majority class. This can lead to overfitting.\n",
    "\n",
    "With this dataset, we will use the undersampling method as the dataset is large enough to not lose much information."
   ],
   "id": "8d5264904d4b48c2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T00:44:31.529310Z",
     "start_time": "2024-11-06T00:44:31.087060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Count the number of fraudulent and non-fraudulent transactions\n",
    "fraud_count = df_scaled.filter(col('Class') == 1).count()\n",
    "non_fraud_count = df_scaled.filter(col('Class') == 0).count()\n",
    "print(f\"Fraudulent transactions: {fraud_count}\\nNon-fraudulent transactions: {non_fraud_count} before balancing\")\n",
    "\n",
    "# Undersampling the non-fraudulent transactions\n",
    "df_fraud = df_scaled.filter(col('Class') == 1)\n",
    "df_non_fraud = df_scaled.filter(col('Class') == 0).sample(fraction=fraud_count/non_fraud_count)\n",
    "\n",
    "# Combine the two classes\n",
    "df_balanced = df_fraud.union(df_non_fraud)\n",
    "\n",
    "# Count the number of fraudulent and non-fraudulent transactions after balancing\n",
    "df_balanced.groupBy('Class').count().show()"
   ],
   "id": "7d371a1828311300",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraudulent transactions: 492\n",
      "Non-fraudulent transactions: 284315 before balancing\n",
      "+-----+-----+\n",
      "|Class|count|\n",
      "+-----+-----+\n",
      "|    1|  492|\n",
      "|    0|  472|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 113
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Splitting the dataset\n",
    "Now that we have completed the preprocessing steps, we can split the dataset into training and testing sets."
   ],
   "id": "550e8e42c53cf389"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T00:44:34.293450Z",
     "start_time": "2024-11-06T00:44:31.554559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df, test_df = df_balanced.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"Training set size: {train_df.count()}\")\n",
    "print(f\"Testing set size: {test_df.count()}\")"
   ],
   "id": "2493b14a53b83940",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 100:============================>                           (8 + 8) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing set size: 183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 114
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T00:44:34.568297Z",
     "start_time": "2024-11-06T00:44:34.367036Z"
    }
   },
   "cell_type": "code",
   "source": "train_df.show(5)",
   "id": "ef8c4519459fd46c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+-----------------+-----------------+----------------+------------------+-----------------+-----------------+-------------------+------------------+------------------+------------------+------------------+------------------+-----------------+--------------------+-----------------+-----------------+-------------------+-----------------+-------------------+-----------------+-------------------+------------------+-------------------+------------------+------------------+-------------------+------------------+------+-----+--------------------+--------------------+\n",
      "|  Time|                 V1|               V2|               V3|              V4|                V5|               V6|               V7|                 V8|                V9|               V10|               V11|               V12|               V13|              V14|                 V15|              V16|              V17|                V18|              V19|                V20|              V21|                V22|               V23|                V24|               V25|               V26|                V27|               V28|Amount|Class|            features|     scaled_features|\n",
      "+------+-------------------+-----------------+-----------------+----------------+------------------+-----------------+-----------------+-------------------+------------------+------------------+------------------+------------------+------------------+-----------------+--------------------+-----------------+-----------------+-------------------+-----------------+-------------------+-----------------+-------------------+------------------+-------------------+------------------+------------------+-------------------+------------------+------+-----+--------------------+--------------------+\n",
      "| 406.0|   -2.3122265423263| 1.95199201064158|-1.60985073229769| 3.9979055875468|-0.522187864667764|-1.42654531920595|-2.53738730624579|   1.39165724829804| -2.77008927719433| -2.77227214465915|  3.20203320709635| -2.89990738849473|-0.595221881324605|-4.28925378244217|   0.389724120274487|-1.14074717980657|-2.83005567450437|-0.0168224681808257|0.416955705037907|  0.126910559061474|0.517232370861764|-0.0350493686052974|-0.465211076182388|  0.320198198514526|0.0445191674731724| 0.177839798284401|  0.261145002567677|-0.143275874698919|   0.0|    1|[406.0,-2.3122265...|[0.00854950202495...|\n",
      "| 472.0|   -3.0435406239976|-3.15730712090228| 1.08846277997285| 2.2886436183814|  1.35980512966107|-1.06482252298131|0.325574266158614|-0.0677936531906277|-0.270952836226548|-0.838586564582682|-0.414575448285725|-0.503140859566824| 0.676501544635863|-1.69202893305906|    2.00063483909015|0.666779695901966|0.599717413841732|   1.72532100745514|0.283344830149495|   2.10233879259444|0.661695924845707|  0.435477208966341|  1.37596574254306| -0.293803152734021| 0.279798031841214|-0.145361714815161| -0.252773122530705|0.0357642251788156| 529.0|    1|[472.0,-3.0435406...|[0.00993932255118...|\n",
      "|6986.0|  -4.39797444171999| 1.35836702839758| -2.5928442182573|2.67978696694832| -1.12813094208956|-1.70653638774951|-3.49619729302467| -0.248777743025673| -0.24776789948008| -4.80163740602813|  4.89584422347523| -10.9128193194019| 0.184371685834387|-6.77109672468083|-0.00732618257771211|-7.35808322132346|-12.5984185405511|  -5.13154862842983|0.308333945758691|  -0.17160787864796|0.573574068424352|  0.176967718048195|-0.436206883597401|-0.0535018648884285| 0.252405261951833|-0.657487754764504| -0.827135714578603| 0.849573379985768|  59.0|    1|[6986.0,-4.397974...|[0.14711039691218...|\n",
      "|7519.0|   1.23423504613468|  3.0197404207034|-4.30459688479665|4.73279513041887|  3.62420083055386|-1.35774566315358| 1.71344498787235| -0.496358487073991| -1.28285782036322| -2.44746925511151|  2.10134386504854|  -4.6096283906446|  1.46437762476188|-6.07933719308005|  -0.339237372732577| 2.58185095378146| 6.73938438478335|   3.04249317830411|-2.72185312222835|0.00906083639534526|-0.37906830709218| -0.704181032215427|-0.656804756348389|  -1.63265295692929|  1.48890144838237| 0.566797273468934|-0.0100162234965625| 0.146792734916988|   1.0|    1|[7519.0,1.2342350...|[0.15833425055578...|\n",
      "|7526.0|0.00843036489558254| 4.13783683497998|-6.24069657194744| 6.6757321631344| 0.768307024571449|-3.35305954788994|-1.63173467271809|   0.15461244822474| -2.79589246446281| -6.18789062970647|  5.66439470857116| -9.85448482287037|-0.306166658250084|-10.6911962118171|  -0.638498192673322|-2.04197379107768|-1.12905587703585|  0.116452521226364|-1.93466573889727|  0.488378221134715| 0.36451420978479| -0.608057133838703|-0.539527941820093|  0.128939982991813|  1.48848121006868|  0.50796267782385|  0.735821636119662| 0.513573740679437|   1.0|    1|[7526.0,0.0084303...|[0.15848165576311...|\n",
      "+------+-------------------+-----------------+-----------------+----------------+------------------+-----------------+-----------------+-------------------+------------------+------------------+------------------+------------------+------------------+-----------------+--------------------+-----------------+-----------------+-------------------+-----------------+-------------------+-----------------+-------------------+------------------+-------------------+------------------+------------------+-------------------+------------------+------+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 115
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T00:44:36.680704Z",
     "start_time": "2024-11-06T00:44:34.592168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import the Random Forest Classifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Create a Random Forest Classifier\n",
    "rf = RandomForestClassifier(featuresCol=\"scaled_features\", labelCol=\"Class\")\n",
    "model = rf.fit(train_df)"
   ],
   "id": "ca130022cbd513c2",
   "outputs": [],
   "execution_count": 116
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T00:44:36.732840Z",
     "start_time": "2024-11-06T00:44:36.707202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Make predictions on the test set\n",
    "predictions = model.transform(test_df)"
   ],
   "id": "589e3e8c3991256e",
   "outputs": [],
   "execution_count": 117
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T00:44:37.091002Z",
     "start_time": "2024-11-06T00:44:36.756707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate the model\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"Class\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ],
   "id": "55ac43642482d48a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.981828060854866\n"
     ]
    }
   ],
   "execution_count": 118
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T00:44:37.307616Z",
     "start_time": "2024-11-06T00:44:37.180709Z"
    }
   },
   "cell_type": "code",
   "source": "predictions.show(5)",
   "id": "90863a583c58b981",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+----------------+------------------+----------------+------------------+-------------------+------------------+------------------+------------------+-----------------+----------------+-----------------+------------------+-----------------+------------------+------------------+-----------------+-----------------+------------------+------------------+------------------+------------------+--------------------+-------------------+------------------+------------------+------------------+------------------+------+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|  Time|                V1|              V2|                V3|              V4|                V5|                 V6|                V7|                V8|                V9|              V10|             V11|              V12|               V13|              V14|               V15|               V16|              V17|              V18|               V19|               V20|               V21|               V22|                 V23|                V24|               V25|               V26|               V27|               V28|Amount|Class|            features|     scaled_features|       rawPrediction|         probability|prediction|\n",
      "+------+------------------+----------------+------------------+----------------+------------------+-------------------+------------------+------------------+------------------+-----------------+----------------+-----------------+------------------+-----------------+------------------+------------------+-----------------+-----------------+------------------+------------------+------------------+------------------+--------------------+-------------------+------------------+------------------+------------------+------------------+------+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|4462.0| -2.30334956758553|  1.759247460267|-0.359744743330052|2.33024305053917|-0.821628328375422|-0.0757875706194599| 0.562319782266954|-0.399146578487216|-0.238253367661746|-1.52541162656194|2.03291215755072|-6.56012429505962|0.0229373234890961|-1.47010153611197|-0.698826068579047| -2.28219382856251|-4.78183085597533|-2.61566494476124| -1.33444106667307|-0.430021867171611|-0.294166317554753|-0.932391057274991|   0.172726295799422|-0.0873295379700724|-0.156114264651172|-0.542627889040196|0.0395659889264757|-0.153028796529788|239.93|    1|[4462.0,-2.303349...|[0.09396029072747...|[2.92690299746751...|[0.14634514987337...|       1.0|\n",
      "|7535.0|0.0267792264491516|4.13246389713003| -6.56059996809658|6.34855667313983|  1.32966566904142|  -2.51347884762413| -1.68910220031328| 0.303252800547589| -3.13940905736457|-6.04546779778801|6.75462544809695|-8.94817857893317| 0.702724998099873|-10.7338541032306| -1.37951985681718| -1.63896011485587|-1.74635013628103|0.776744097926754| -1.32735663549015| 0.587743219006407| 0.370508651493253| -0.57675247317433|  -0.669605371766238| -0.759907529538618|  1.60505555017462| 0.540675396428899| 0.737040381683977| 0.496699108168337|   1.0|    1|[7535.0,0.0267792...|[0.15867117674396...|          [0.0,20.0]|           [0.0,1.0]|       1.0|\n",
      "|7551.0| 0.316459000444982|3.80907594667829| -5.61515901119457|6.04744510216478|  1.55402595692572|   -2.6513531120137|-0.746579273100222|0.0555863112529252|  -2.6786785422399|-4.95949291161496|6.43905335158373|-7.52011739288703|  0.38635166741077|-9.25230724747513| -1.36518841502051|-0.502362190618164|0.784426598154274| 1.49430460743838| -1.80801215867357| 0.388307428238927| 0.208828369001674|-0.511746619200722|  -0.583813220813723| -0.219845029091423|  1.47475258440688| 0.491191925656006| 0.518868284577287|  0.40252806767232|   1.0|    1|[7551.0,0.3164590...|[0.15900810293213...|[0.00892857142857...|[4.46428571428571...|       1.0|\n",
      "|8090.0| -1.78322883722709|3.40279371307631| -3.82274226552205|2.62536815276644| -1.97641541136284|  -2.73168901231308| -3.43055914949085|  1.41320351399318|-0.776941494554916|-6.19988176274188|4.36671348631445|-8.24326243368313| 0.345761165232579|-6.59055029705192| 0.265576095932165| -3.02845238846152|-4.21448627485607|-1.21360784773725|-0.265421595374812| 0.364089115014408| 0.454031931736316|-0.577525824201459|  0.0459665375362652|  0.461700017592254| 0.044146161217817| 0.305704433918549| 0.530980986346765| 0.243745598122111|   1.0|    1|[8090.0,-1.783228...|[0.17035830389630...|          [0.0,20.0]|           [0.0,1.0]|       1.0|\n",
      "|8614.0| -2.16992897559082| 3.6396539992044| -4.50849778617728|2.73066814884895| -2.12269287053048|  -2.34101682766366| -4.23525308290613|  1.70353765763508| -1.30527913344307| -6.7167200227127|6.35361231915591|-8.60164826276464| 0.449930038237225|-7.50616937408145|-0.438081781812467| -3.69451599830956|-6.30475338603049|-1.26758712788115| 0.357987030484253|   0.5007790539507| 0.645103275621378|-0.503529448911197|-5.22821818299227E-4| 0.0716957787676742|0.0920074334247208| 0.308497931577036| 0.552590914653841| 0.298954478709399|   1.0|    1|[8614.0,-2.169928...|[0.18139263655905...|          [0.0,20.0]|           [0.0,1.0]|       1.0|\n",
      "+------+------------------+----------------+------------------+----------------+------------------+-------------------+------------------+------------------+------------------+-----------------+----------------+-----------------+------------------+-----------------+------------------+------------------+-----------------+-----------------+------------------+------------------+------------------+------------------+--------------------+-------------------+------------------+------------------+------------------+------------------+------+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 119
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T01:00:30.010679Z",
     "start_time": "2024-11-06T01:00:29.188380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Plot ROC curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "# Check that 'probability' is of type Vector\n",
    "print(predictions.select(\"probability\").dtypes)\n",
    "\n",
    "# Extract prediction probabilities (for plotting the ROC curve)\n",
    "predictions = predictions.withColumn(\"probability\", vector_to_array(predictions[\"probability\"]))\n",
    "\n",
    "# Collect predictions and probabilities as RDD\n",
    "prediction_and_labels = predictions.select('prediction', 'Class', 'probability').rdd.map(\n",
    "    lambda row: (row['probability'][1], row['Class'], row['prediction'])  # Get the positive class probability\n",
    ")\n",
    "\n",
    "# Calculate ROC using BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='Class', rawPredictionCol='prediction')\n",
    "roc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
    "print(f\"AUC: {roc}\")\n",
    "\n",
    "\n",
    "# # Convert the predictions to a Pandas DataFrame\n",
    "# predictions_df = predictions.select('Class', 'probability').toPandas()\n",
    "# \n",
    "# # Get the false positive rate and true positive rate\n",
    "# fpr, tpr, _ = roc_curve(predictions_df['Class'], predictions_df['probability'].apply(lambda x: x[1]))\n",
    "# \n",
    "# # Calculate the AUC\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "# \n",
    "# # Plot the ROC curve\n",
    "# plt.figure()\n",
    "# plt.plot(fpr, tpr, color='darkorange', lw=1, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver Operating Characteristic')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.show()\n"
   ],
   "id": "3395c97e6b459511",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('probability', 'array<double>')]\n",
      "AUC: 0.9504950495049505\n"
     ]
    }
   ],
   "execution_count": 143
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T01:00:34.546286Z",
     "start_time": "2024-11-06T01:00:34.233692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "metrics = BinaryClassificationMetrics(prediction_and_labels)\n",
    "roc = metrics.roc().collect()"
   ],
   "id": "344d0624eb9f2170",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/pyspark/sql/context.py:157: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "24/11/05 20:00:34 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 810, in main\n",
      "    eval_type = read_int(infile)\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:179)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (functions$$$Lambda$5339/0x0000000801a5e840: (array<double>) => array<double>).\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "Caused by: java.lang.IllegalArgumentException: function vector_to_array requires a non-null input argument and input type must be `org.apache.spark.ml.linalg.Vector` or `org.apache.spark.mllib.linalg.Vector`, but got scala.collection.mutable.WrappedArray$ofRef.\n",
      "\tat org.apache.spark.ml.functions$.$anonfun$vectorToArrayUdf$1(functions.scala:37)\n",
      "\t... 14 more\n",
      "24/11/05 20:00:34 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (functions$$$Lambda$5339/0x0000000801a5e840: (array<double>) => array<double>).\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "Caused by: java.lang.IllegalArgumentException: function vector_to_array requires a non-null input argument and input type must be `org.apache.spark.ml.linalg.Vector` or `org.apache.spark.mllib.linalg.Vector`, but got scala.collection.mutable.WrappedArray$ofRef.\n",
      "\tat org.apache.spark.ml.functions$.$anonfun$vectorToArrayUdf$1(functions.scala:37)\n",
      "\t... 14 more\n",
      "24/11/05 20:00:34 ERROR Executor: Exception in task 0.0 in stage 234.0 (TID 1507)\n",
      "org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (functions$$$Lambda$5339/0x0000000801a5e840: (array<double>) => array<double>).\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "Caused by: java.lang.IllegalArgumentException: function vector_to_array requires a non-null input argument and input type must be `org.apache.spark.ml.linalg.Vector` or `org.apache.spark.mllib.linalg.Vector`, but got scala.collection.mutable.WrappedArray$ofRef.\n",
      "\tat org.apache.spark.ml.functions$.$anonfun$vectorToArrayUdf$1(functions.scala:37)\n",
      "\t... 14 more\n",
      "24/11/05 20:00:34 WARN TaskSetManager: Lost task 0.0 in stage 234.0 (TID 1507) (10.0.0.206 executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (functions$$$Lambda$5339/0x0000000801a5e840: (array<double>) => array<double>).\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "Caused by: java.lang.IllegalArgumentException: function vector_to_array requires a non-null input argument and input type must be `org.apache.spark.ml.linalg.Vector` or `org.apache.spark.mllib.linalg.Vector`, but got scala.collection.mutable.WrappedArray$ofRef.\n",
      "\tat org.apache.spark.ml.functions$.$anonfun$vectorToArrayUdf$1(functions.scala:37)\n",
      "\t... 14 more\n",
      "\n",
      "24/11/05 20:00:34 ERROR TaskSetManager: Task 0 in stage 234.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 234.0 failed 1 times, most recent failure: Lost task 0.0 in stage 234.0 (TID 1507) (10.0.0.206 executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (functions$$$Lambda$5339/0x0000000801a5e840: (array<double>) => array<double>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\nCaused by: java.lang.IllegalArgumentException: function vector_to_array requires a non-null input argument and input type must be `org.apache.spark.ml.linalg.Vector` or `org.apache.spark.mllib.linalg.Vector`, but got scala.collection.mutable.WrappedArray$ofRef.\n\tat org.apache.spark.ml.functions$.$anonfun$vectorToArrayUdf$1(functions.scala:37)\n\t... 14 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:179)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (functions$$$Lambda$5339/0x0000000801a5e840: (array<double>) => array<double>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\nCaused by: java.lang.IllegalArgumentException: function vector_to_array requires a non-null input argument and input type must be `org.apache.spark.ml.linalg.Vector` or `org.apache.spark.mllib.linalg.Vector`, but got scala.collection.mutable.WrappedArray$ofRef.\n\tat org.apache.spark.ml.functions$.$anonfun$vectorToArrayUdf$1(functions.scala:37)\n\t... 14 more\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[144], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m metrics \u001B[38;5;241m=\u001B[39m BinaryClassificationMetrics(prediction_and_labels)\n\u001B[1;32m      2\u001B[0m roc \u001B[38;5;241m=\u001B[39m metrics\u001B[38;5;241m.\u001B[39mroc()\u001B[38;5;241m.\u001B[39mcollect()\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/mllib/evaluation.py:73\u001B[0m, in \u001B[0;36mBinaryClassificationMetrics.__init__\u001B[0;34m(self, scoreAndLabels)\u001B[0m\n\u001B[1;32m     71\u001B[0m sc \u001B[38;5;241m=\u001B[39m scoreAndLabels\u001B[38;5;241m.\u001B[39mctx\n\u001B[1;32m     72\u001B[0m sql_ctx \u001B[38;5;241m=\u001B[39m SQLContext\u001B[38;5;241m.\u001B[39mgetOrCreate(sc)\n\u001B[0;32m---> 73\u001B[0m numCol \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(scoreAndLabels\u001B[38;5;241m.\u001B[39mfirst())\n\u001B[1;32m     74\u001B[0m schema \u001B[38;5;241m=\u001B[39m StructType(\n\u001B[1;32m     75\u001B[0m     [\n\u001B[1;32m     76\u001B[0m         StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mscore\u001B[39m\u001B[38;5;124m\"\u001B[39m, DoubleType(), nullable\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m),\n\u001B[1;32m     77\u001B[0m         StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m\"\u001B[39m, DoubleType(), nullable\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m),\n\u001B[1;32m     78\u001B[0m     ]\n\u001B[1;32m     79\u001B[0m )\n\u001B[1;32m     80\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m numCol \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m3\u001B[39m:\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/rdd.py:2869\u001B[0m, in \u001B[0;36mRDD.first\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   2843\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfirst\u001B[39m(\u001B[38;5;28mself\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRDD[T]\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m T:\n\u001B[1;32m   2844\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   2845\u001B[0m \u001B[38;5;124;03m    Return the first element in this RDD.\u001B[39;00m\n\u001B[1;32m   2846\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2867\u001B[0m \u001B[38;5;124;03m    ValueError: RDD is empty\u001B[39;00m\n\u001B[1;32m   2868\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 2869\u001B[0m     rs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtake(\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m   2870\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m rs:\n\u001B[1;32m   2871\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m rs[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/rdd.py:2836\u001B[0m, in \u001B[0;36mRDD.take\u001B[0;34m(self, num)\u001B[0m\n\u001B[1;32m   2833\u001B[0m         taken \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   2835\u001B[0m p \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mrange\u001B[39m(partsScanned, \u001B[38;5;28mmin\u001B[39m(partsScanned \u001B[38;5;241m+\u001B[39m numPartsToTry, totalParts))\n\u001B[0;32m-> 2836\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontext\u001B[38;5;241m.\u001B[39mrunJob(\u001B[38;5;28mself\u001B[39m, takeUpToNumLeft, p)\n\u001B[1;32m   2838\u001B[0m items \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m res\n\u001B[1;32m   2839\u001B[0m partsScanned \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m numPartsToTry\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/context.py:2319\u001B[0m, in \u001B[0;36mSparkContext.runJob\u001B[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001B[0m\n\u001B[1;32m   2317\u001B[0m mappedRDD \u001B[38;5;241m=\u001B[39m rdd\u001B[38;5;241m.\u001B[39mmapPartitions(partitionFunc)\n\u001B[1;32m   2318\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 2319\u001B[0m sock_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mPythonRDD\u001B[38;5;241m.\u001B[39mrunJob(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsc\u001B[38;5;241m.\u001B[39msc(), mappedRDD\u001B[38;5;241m.\u001B[39m_jrdd, partitions)\n\u001B[1;32m   2320\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(_load_from_socket(sock_info, mappedRDD\u001B[38;5;241m.\u001B[39m_jrdd_deserializer))\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1323\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:169\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    167\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    168\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 169\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39ma, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\n\u001B[1;32m    170\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    171\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
      "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 234.0 failed 1 times, most recent failure: Lost task 0.0 in stage 234.0 (TID 1507) (10.0.0.206 executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (functions$$$Lambda$5339/0x0000000801a5e840: (array<double>) => array<double>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\nCaused by: java.lang.IllegalArgumentException: function vector_to_array requires a non-null input argument and input type must be `org.apache.spark.ml.linalg.Vector` or `org.apache.spark.mllib.linalg.Vector`, but got scala.collection.mutable.WrappedArray$ofRef.\n\tat org.apache.spark.ml.functions$.$anonfun$vectorToArrayUdf$1(functions.scala:37)\n\t... 14 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:179)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (functions$$$Lambda$5339/0x0000000801a5e840: (array<double>) => array<double>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\nCaused by: java.lang.IllegalArgumentException: function vector_to_array requires a non-null input argument and input type must be `org.apache.spark.ml.linalg.Vector` or `org.apache.spark.mllib.linalg.Vector`, but got scala.collection.mutable.WrappedArray$ofRef.\n\tat org.apache.spark.ml.functions$.$anonfun$vectorToArrayUdf$1(functions.scala:37)\n\t... 14 more\n"
     ]
    }
   ],
   "execution_count": 144
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

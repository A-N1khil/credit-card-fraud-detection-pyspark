{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T23:13:54.746054Z",
     "start_time": "2024-11-05T23:13:54.422560Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Spark session and functions\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, isnan, when, count\n",
    "\n",
    "# Machine Learning imports\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f285463abddc3d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T23:13:56.674729Z",
     "start_time": "2024-11-05T23:13:54.768567Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/05 18:13:55 WARN Utils: Your hostname, Nikhils-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.0.0.206 instead (on interface en0)\n",
      "24/11/05 18:13:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/05 18:13:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('CreditCardFraudDetection').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c05cece7ea78e36",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "The dataset is available at [Kaggle](https://www.kaggle.com/mlg-ulb/creditcardfraud)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a0a8627542ecf38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T23:13:59.753623Z",
     "start_time": "2024-11-05T23:13:56.726682Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('dataset/creditcard.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa63ab4b9193d7f",
   "metadata": {},
   "source": [
    "## Explaining the dataset\n",
    "The dataset contains transactions made by credit cards in September 2013 by european cardholders. There are 28 features, all of them are numerical. The features V1, V2, ... V28 are the result of a PCA transformation. The only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.  \n",
    "V1, V2, ... V28 are the principal components obtained with PCA. They have been anonymized to protect sensitive information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0bf4f3ceb117e50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T23:13:59.786349Z",
     "start_time": "2024-11-05T23:13:59.780769Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Time: double (nullable = true)\n",
      " |-- V1: double (nullable = true)\n",
      " |-- V2: double (nullable = true)\n",
      " |-- V3: double (nullable = true)\n",
      " |-- V4: double (nullable = true)\n",
      " |-- V5: double (nullable = true)\n",
      " |-- V6: double (nullable = true)\n",
      " |-- V7: double (nullable = true)\n",
      " |-- V8: double (nullable = true)\n",
      " |-- V9: double (nullable = true)\n",
      " |-- V10: double (nullable = true)\n",
      " |-- V11: double (nullable = true)\n",
      " |-- V12: double (nullable = true)\n",
      " |-- V13: double (nullable = true)\n",
      " |-- V14: double (nullable = true)\n",
      " |-- V15: double (nullable = true)\n",
      " |-- V16: double (nullable = true)\n",
      " |-- V17: double (nullable = true)\n",
      " |-- V18: double (nullable = true)\n",
      " |-- V19: double (nullable = true)\n",
      " |-- V20: double (nullable = true)\n",
      " |-- V21: double (nullable = true)\n",
      " |-- V22: double (nullable = true)\n",
      " |-- V23: double (nullable = true)\n",
      " |-- V24: double (nullable = true)\n",
      " |-- V25: double (nullable = true)\n",
      " |-- V26: double (nullable = true)\n",
      " |-- V27: double (nullable = true)\n",
      " |-- V28: double (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      " |-- Class: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34b0d03e1adf2bd",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "## Checking for missing values\n",
    "First, we check for missing values in the dataset. If there are any, we will clean them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb4438c9168c3de6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T23:14:01.261713Z",
     "start_time": "2024-11-05T23:13:59.815548Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/05 18:13:59 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 2:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+------+-----+\n",
      "|Time| V1| V2| V3| V4| V5| V6| V7| V8| V9|V10|V11|V12|V13|V14|V15|V16|V17|V18|V19|V20|V21|V22|V23|V24|V25|V26|V27|V28|Amount|Class|\n",
      "+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+------+-----+\n",
      "|   0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|     0|    0|\n",
      "+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Checking for missing values\n",
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "977ca2ffc58c6128",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T23:14:02.453868Z",
     "start_time": "2024-11-05T23:14:01.323496Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original size: 284807\n",
      "Cleaned size: 284807\n"
     ]
    }
   ],
   "source": [
    "original_size = df.count()\n",
    "print(f'Original size: {original_size}')\n",
    "\n",
    "# Clean up missing values\n",
    "df_cleaned = df.na.drop()\n",
    "\n",
    "cleaned_size = df_cleaned.count()\n",
    "print(f'Cleaned size: {cleaned_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c16b0e1a448f807",
   "metadata": {},
   "source": [
    "## Assembling the features\n",
    "In PySpark MLlib, all input features should be combined into a single vector column. To do this, you use the `VectorAssembler`.  \n",
    "This is a transformer that combines a given list of columns into a single vector column. It is useful for combining raw features and features generated by different feature transformers into a single feature vector, in order to train ML models like logistic regression and decision trees.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "616f52ac0c1137bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T23:14:03.611452Z",
     "start_time": "2024-11-05T23:14:03.303881Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|features                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |Class|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|[0.0,-1.3598071336738,-0.0727811733098497,2.53634673796914,1.37815522427443,-0.338320769942518,0.462387777762292,0.239598554061257,0.0986979012610507,0.363786969611213,0.0907941719789316,-0.551599533260813,-0.617800855762348,-0.991389847235408,-0.311169353699879,1.46817697209427,-0.470400525259478,0.207971241929242,0.0257905801985591,0.403992960255733,0.251412098239705,-0.018306777944153,0.277837575558899,-0.110473910188767,0.0669280749146731,0.128539358273528,-0.189114843888824,0.133558376740387,-0.0210530534538215,149.62]|0    |\n",
      "|[0.0,1.19185711131486,0.26615071205963,0.16648011335321,0.448154078460911,0.0600176492822243,-0.0823608088155687,-0.0788029833323113,0.0851016549148104,-0.255425128109186,-0.166974414004614,1.61272666105479,1.06523531137287,0.48909501589608,-0.143772296441519,0.635558093258208,0.463917041022171,-0.114804663102346,-0.183361270123994,-0.145783041325259,-0.0690831352230203,-0.225775248033138,-0.638671952771851,0.101288021253234,-0.339846475529127,0.167170404418143,0.125894532368176,-0.00898309914322813,0.0147241691924927,2.69]|0    |\n",
      "|[1.0,-1.35835406159823,-1.34016307473609,1.77320934263119,0.379779593034328,-0.503198133318193,1.80049938079263,0.791460956450422,0.247675786588991,-1.51465432260583,0.207642865216696,0.624501459424895,0.066083685268831,0.717292731410831,-0.165945922763554,2.34586494901581,-2.89008319444231,1.10996937869599,-0.121359313195888,-2.26185709530414,0.524979725224404,0.247998153469754,0.771679401917229,0.909412262347719,-0.689280956490685,-0.327641833735251,-0.139096571514147,-0.0553527940384261,-0.0597518405929204,378.66]       |0    |\n",
      "|[1.0,-0.966271711572087,-0.185226008082898,1.79299333957872,-0.863291275036453,-0.0103088796030823,1.24720316752486,0.23760893977178,0.377435874652262,-1.38702406270197,-0.0549519224713749,-0.226487263835401,0.178228225877303,0.507756869957169,-0.28792374549456,-0.631418117709045,-1.0596472454325,-0.684092786345479,1.96577500349538,-1.2326219700892,-0.208037781160366,-0.108300452035545,0.00527359678253453,-0.190320518742841,-1.17557533186321,0.647376034602038,-0.221928844458407,0.0627228487293033,0.0614576285006353,123.5]  |0    |\n",
      "|[2.0,-1.15823309349523,0.877736754848451,1.548717846511,0.403033933955121,-0.407193377311653,0.0959214624684256,0.592940745385545,-0.270532677192282,0.817739308235294,0.753074431976354,-0.822842877946363,0.53819555014995,1.3458515932154,-1.11966983471731,0.175121130008994,-0.451449182813529,-0.237033239362776,-0.0381947870352842,0.803486924960175,0.408542360392758,-0.00943069713232919,0.79827849458971,-0.137458079619063,0.141266983824769,-0.206009587619756,0.502292224181569,0.219422229513348,0.215153147499206,69.99]        |0    |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select feature columns (all except 'Class', which is the label)\n",
    "feature_columns = df_cleaned.columns[:-1]  # Assuming the last column is the label\n",
    "\n",
    "# Assemble feature columns into a single vector column called \"features\"\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "df_transformed = assembler.transform(df_cleaned)\n",
    "\n",
    "# Show the transformed data\n",
    "df_transformed.select(\"features\", \"Class\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b91a3b0ba4c48f",
   "metadata": {},
   "source": [
    "## Scaling the features\n",
    "Feature scaling is important because some machine learning algorithms are sensitive to the scale of input data. You can scale features using the `StandardScaler`.\n",
    "StandardScaler transforms a dataset of Vector rows, normalizing each feature to have unit standard deviation and/or zero mean. It takes parameters:\n",
    "- `inputCol`: input column name.\n",
    "- `outputCol`: output column name.\n",
    "- `withStd`: True by default. Scales the data to unit standard deviation.\n",
    "- `withMean`: False by default. Centers the data with mean before scaling. It will build a dense output, so take care when applying to sparse input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e8dacd89c420414",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T23:08:59.033374Z",
     "start_time": "2024-11-05T23:08:58.107345Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: inputCol: input column name. (current: features)\n",
      "outputCol: output column name. (default: StandardScaler_a04f4c59bc97__output, current: scaled_features)\n",
      "withMean: Center data with mean (default: False, current: False)\n",
      "withStd: Scale to unit standard deviation (default: True, current: True)\n",
      "+--------------------+-----+\n",
      "|     scaled_features|Class|\n",
      "+--------------------+-----+\n",
      "|[0.0,-0.694241102...|    0|\n",
      "|[0.0,0.6084952594...|    0|\n",
      "|[2.10578867609768...|    0|\n",
      "|[2.10578867609768...|    0|\n",
      "|[4.21157735219537...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=False)\n",
    "df_scaled = scaler.fit(df_transformed).transform(df_transformed)\n",
    "\n",
    "# Summary of the scaling process\n",
    "print(f\"Mean: {scaler.explainParams()}\")\n",
    "# print(f\"Standard Deviation: {scaler.std}\")\n",
    "\n",
    "# Show the scaled data\n",
    "df_scaled.select(\"scaled_features\", \"Class\").show(5)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Class distribution\n",
    "We group the dataset by the 'Class' column to check the distribution of the classes."
   ],
   "id": "9f728ca8a7bddc2c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T23:38:34.730391Z",
     "start_time": "2024-11-05T23:38:33.527009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Checking for class distribution\n",
    "df_cleaned.groupBy('Class').count().show()"
   ],
   "id": "a22ae163d7d9e44e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|Class| count|\n",
      "+-----+------+\n",
      "|    1|   492|\n",
      "|    0|284315|\n",
      "+-----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Handling Class imbalance\n",
    "From the class distribution, we can see that the dataset is highly imbalanced. The number of non-fraudulent transactions is much higher than the number of fraudulent transactions. For our model to learn effectively, we need to balance the classes.  \n",
    "This can be done in two ways:\n",
    "1. **Undersampling**: In this method, we randomly sample the majority class to match the minority class. This can lead to loss of information.\n",
    "2. **Oversampling**: In this method, we randomly duplicate the minority class to match the majority class. This can lead to overfitting.\n",
    "\n",
    "With this dataset, we will use the undersampling method as the dataset is large enough to not lose much information."
   ],
   "id": "8d5264904d4b48c2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T23:43:28.602878Z",
     "start_time": "2024-11-05T23:43:26.763854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Count the number of fraudulent and non-fraudulent transactions\n",
    "fraud_count = df_cleaned.filter(col('Class') == 1).count()\n",
    "non_fraud_count = df_cleaned.filter(col('Class') == 0).count()\n",
    "print(f\"Fraudulent transactions: {fraud_count}\\nNon-fraudulent transactions: {non_fraud_count} before balancing\")\n",
    "\n",
    "# Undersampling the non-fraudulent transactions\n",
    "df_fraud = df_cleaned.filter(col('Class') == 1)\n",
    "df_non_fraud = df_cleaned.filter(col('Class') == 0).sample(False, fraud_count/non_fraud_count)\n",
    "\n",
    "# Combine the two classes\n",
    "df_balanced = df_fraud.union(df_non_fraud)\n",
    "\n",
    "# Count the number of fraudulent and non-fraudulent transactions after balancing\n",
    "df_balanced.groupBy('Class').count().show()"
   ],
   "id": "7d371a1828311300",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraudulent transactions: 492\n",
      "Non-fraudulent transactions: 284315 before balancing\n",
      "+-----+-----+\n",
      "|Class|count|\n",
      "+-----+-----+\n",
      "|    1|  492|\n",
      "|    0|  450|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Splitting the dataset\n",
    "Now that we have completed the preprocessing steps, we can split the dataset into training and testing sets."
   ],
   "id": "550e8e42c53cf389"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2493b14a53b83940"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

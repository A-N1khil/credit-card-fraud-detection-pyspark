{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-07T01:42:38.157643Z",
     "start_time": "2024-12-07T01:42:38.079814Z"
    }
   },
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType\n",
    "\n",
    "from logs.CustomLogger import CustomLogger\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We need to set the environment variables before creating the Spark session. We can do this by setting the `PYSPARK_SUBMIT_ARGS` environment variable to include the necessary Kafka package. We can then create a Spark session using the `SparkSession` class.",
   "id": "b0835b41d35e3de2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T01:42:38.180019Z",
     "start_time": "2024-12-07T01:42:38.167149Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set the necessary Spark environment variables\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1 pyspark-shell'"
   ],
   "id": "8369743b0dd52c21",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Setup the Logger",
   "id": "a0a13167bf697ae6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T01:42:38.210516Z",
     "start_time": "2024-12-07T01:42:38.191979Z"
    }
   },
   "cell_type": "code",
   "source": "logger = CustomLogger(\"KafkaConsumer2\")",
   "id": "678bd9c79d41a25a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:KafkaConsumer2:Logger is set up\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logger is set up. Check producer.log for logs.\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T01:42:38.291570Z",
     "start_time": "2024-12-07T01:42:38.237510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaConsumer2\") \\\n",
    "    .config(\"spark.kafka.consumer.partition.assignment.strategy\", \"org.apache.kafka.clients.consumer.RoundRobinAssignor\") \\\n",
    "    .getOrCreate()"
   ],
   "id": "d8968635ac3ac996",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Create a schema for the data\n",
    "The data we are going to consume from Kafka is a JSON string that contains the following fields"
   ],
   "id": "ee816b0395e7986c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T01:42:38.312799Z",
     "start_time": "2024-12-07T01:42:38.301244Z"
    }
   },
   "cell_type": "code",
   "source": [
    "schema = StructType([\n",
    "    StructField(\"Time\", DoubleType(), True),\n",
    "    StructField(\"V1\", DoubleType(), True),\n",
    "    StructField(\"V2\", DoubleType(), True),\n",
    "    StructField(\"V3\", DoubleType(), True),\n",
    "    StructField(\"V4\", DoubleType(), True),\n",
    "    StructField(\"V5\", DoubleType(), True),\n",
    "    StructField(\"V6\", DoubleType(), True),\n",
    "    StructField(\"V7\", DoubleType(), True),\n",
    "    StructField(\"V8\", DoubleType(), True),\n",
    "    StructField(\"V9\", DoubleType(), True),\n",
    "    StructField(\"V10\", DoubleType(), True),\n",
    "    StructField(\"V11\", DoubleType(), True),\n",
    "    StructField(\"V12\", DoubleType(), True),\n",
    "    StructField(\"V13\", DoubleType(), True),\n",
    "    StructField(\"V14\", DoubleType(), True),\n",
    "    StructField(\"V15\", DoubleType(), True),\n",
    "    StructField(\"V16\", DoubleType(), True),\n",
    "    StructField(\"V17\", DoubleType(), True),\n",
    "    StructField(\"V18\", DoubleType(), True),\n",
    "    StructField(\"V19\", DoubleType(), True),\n",
    "    StructField(\"V20\", DoubleType(), True),\n",
    "    StructField(\"V21\", DoubleType(), True),\n",
    "    StructField(\"V22\", DoubleType(), True),\n",
    "    StructField(\"V23\", DoubleType(), True),\n",
    "    StructField(\"V24\", DoubleType(), True),\n",
    "    StructField(\"V25\", DoubleType(), True),\n",
    "    StructField(\"V26\", DoubleType(), True),\n",
    "    StructField(\"V27\", DoubleType(), True),\n",
    "    StructField(\"V28\", DoubleType(), True),\n",
    "    StructField(\"Amount\", DoubleType(), True),\n",
    "    StructField(\"Class\", StringType(), True)\n",
    "])\n",
    "logger.info(f\"Schema created: {schema}\")"
   ],
   "id": "23f2a63df8d572e9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:KafkaConsumer2:Schema created: StructType([StructField('Time', DoubleType(), True), StructField('V1', DoubleType(), True), StructField('V2', DoubleType(), True), StructField('V3', DoubleType(), True), StructField('V4', DoubleType(), True), StructField('V5', DoubleType(), True), StructField('V6', DoubleType(), True), StructField('V7', DoubleType(), True), StructField('V8', DoubleType(), True), StructField('V9', DoubleType(), True), StructField('V10', DoubleType(), True), StructField('V11', DoubleType(), True), StructField('V12', DoubleType(), True), StructField('V13', DoubleType(), True), StructField('V14', DoubleType(), True), StructField('V15', DoubleType(), True), StructField('V16', DoubleType(), True), StructField('V17', DoubleType(), True), StructField('V18', DoubleType(), True), StructField('V19', DoubleType(), True), StructField('V20', DoubleType(), True), StructField('V21', DoubleType(), True), StructField('V22', DoubleType(), True), StructField('V23', DoubleType(), True), StructField('V24', DoubleType(), True), StructField('V25', DoubleType(), True), StructField('V26', DoubleType(), True), StructField('V27', DoubleType(), True), StructField('V28', DoubleType(), True), StructField('Amount', DoubleType(), True), StructField('Class', StringType(), True)])\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Consume data from Kafka\n",
    "We can consume data from Kafka using the `readStream` method of the `SparkSession` object. We need to specify the Kafka server and the topic to consume data from. We can then parse the value as JSON (if applicable) and write the parsed data to the console."
   ],
   "id": "a080a4292d3af620"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T01:42:38.457752Z",
     "start_time": "2024-12-07T01:42:38.332059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"distributed_transactions\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .option(\"kafka.group.id\", \"distributed_transactions\") \\\n",
    "    .load()"
   ],
   "id": "90c45f57ea77bed7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/06 20:42:38 WARN KafkaSourceProvider: Kafka option 'kafka.group.id' has been set on this query, it is\n",
      " not recommended to set this option. This option is unsafe to use since multiple concurrent\n",
      " queries or sources using the same group id will interfere with each other as they are part\n",
      " of the same consumer group. Restarted queries may also suffer interference from the\n",
      " previous run having the same group id. The user should have only one query per group id,\n",
      " and/or set the option 'kafka.session.timeout.ms' to be very small so that the Kafka\n",
      " consumers from the previous query are marked dead by the Kafka group coordinator before the\n",
      " restarted query starts running.\n",
      "    \n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T01:42:38.469254Z",
     "start_time": "2024-12-07T01:42:38.461288Z"
    }
   },
   "cell_type": "code",
   "source": "# spark.stop()",
   "id": "8868f527f0fd165",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T01:42:38.552184Z",
     "start_time": "2024-12-07T01:42:38.480931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Parse the value as JSON (if applicable)\n",
    "parsed_df = df.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")"
   ],
   "id": "56674279dfd6bf6a",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Getting parsed data from Kafka\n",
    "In order to use the transactions data in order for our model to run predictions, we need to save the parsed data in a form that our pipeline can understand and use. We can do this by saving the parsed data"
   ],
   "id": "ca1b73cd23a0a68a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T01:42:39.990435Z",
     "start_time": "2024-12-07T01:42:38.564021Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pipeline.pipeline import CustomPipeline\n",
    "\n",
    "model_path = \"./../models/credit_card_fraud_detection_model\"\n",
    "pipeline_path = \"./../pipeline/credit_card_fraud_detection_pipeline\"\n",
    "pipeline = CustomPipeline(model_path, pipeline_path)\n",
    "cols = ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
    "        'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
    "        'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']\n",
    "pipeline.create_pipeline(cols=cols)\n",
    "# predictions = []"
   ],
   "id": "33d17599f1f35017",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pipeline model from ./../pipeline/credit_card_fraud_detection_pipeline\n",
      "Pipeline created.\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Predictions\n",
    "First, we create an empty dataframe to hold our predictions. We then define a function that processes the records in the parsed data. The function takes a dataframe and an epoch id as arguments. We then collect the rows from the dataframe and check if there are any rows. If there are rows, we transform the dataframe using the pipeline and collect the rows from the predictions. We then print the predictions."
   ],
   "id": "3d4da630eb6cf81a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T01:47:27.028826Z",
     "start_time": "2024-12-07T01:47:26.893928Z"
    }
   },
   "cell_type": "code",
   "source": "prediction_records_holder = [spark.createDataFrame([], StructType([]))]",
   "id": "50667db68edaea01",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T01:47:28.678176Z",
     "start_time": "2024-12-07T01:47:28.643684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from functools import partial\n",
    "\n",
    "def process_record(df, id, prediction_records_holder, verbose=False):\n",
    "    rows = df.collect()\n",
    "    logger.info(f\"Processing {rows} records\")\n",
    "    if len(rows) != 0:\n",
    "        # Do no do anything if there are no rows\n",
    "        predictions = pipeline.transform(df)\n",
    "        logger.info(f\"Predictions: {predictions}\")\n",
    "\n",
    "        # Append the predictions to the prediction_records DataFrame\n",
    "        updated_prediction_records = prediction_records_holder[0].unionByName(predictions, allowMissingColumns=True)\n",
    "        prediction_records_holder[0] = updated_prediction_records\n",
    "\n",
    "        # Print or process the predictions if verbose is set to True\n",
    "        if verbose:\n",
    "            # Convert the predictions DataFrame to a list of dictionaries\n",
    "            rows = predictions.collect()\n",
    "            for row in rows:\n",
    "                print(row.asDict())\n",
    "\n",
    "process_record_with_params = partial(process_record, prediction_records_holder=prediction_records_holder, verbose=False)"
   ],
   "id": "d19329c1f9644873",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Set `verbose=True` to print the predictions as they happen",
   "id": "d87d439d4452faa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T01:47:33.877298Z",
     "start_time": "2024-12-07T01:47:30.841844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Modify the query to use foreachBatch\n",
    "query = parsed_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .foreachBatch(process_record_with_params) \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ],
   "id": "d7eeaef353653f30",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/06 20:47:30 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/fn/0_yk91x94834b2xtx5nmcsqh0000gn/T/temporary-69285653-af75-4c33-8433-0b841be37e00. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/12/06 20:47:30 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/12/06 20:47:30 WARN KafkaSourceProvider: Kafka option 'kafka.group.id' has been set on this query, it is\n",
      " not recommended to set this option. This option is unsafe to use since multiple concurrent\n",
      " queries or sources using the same group id will interfere with each other as they are part\n",
      " of the same consumer group. Restarted queries may also suffer interference from the\n",
      " previous run having the same group id. The user should have only one query per group id,\n",
      " and/or set the option 'kafka.session.timeout.ms' to be very small so that the Kafka\n",
      " consumers from the previous query are marked dead by the Kafka group coordinator before the\n",
      " restarted query starts running.\n",
      "    \n",
      "24/12/06 20:47:31 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, group.id, auto.offset.reset]' were supplied but are not used yet.\n",
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "    \n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "    \n",
      "  File \"/opt/anaconda3/lib/python3.11/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "    \n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[34], line 7\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Modify the query to use foreachBatch\u001B[39;00m\n\u001B[1;32m      2\u001B[0m query \u001B[38;5;241m=\u001B[39m parsed_df\u001B[38;5;241m.\u001B[39mwriteStream \\\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;241m.\u001B[39moutputMode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mappend\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;241m.\u001B[39mforeachBatch(process_record_with_params) \\\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n\u001B[0;32m----> 7\u001B[0m query\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/sql/streaming/query.py:201\u001B[0m, in \u001B[0;36mStreamingQuery.awaitTermination\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    199\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination(\u001B[38;5;28mint\u001B[39m(timeout \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m))\n\u001B[1;32m    200\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 201\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1314\u001B[0m args_command, temp_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_args(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m-> 1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m   1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1323\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1038\u001B[0m, in \u001B[0;36mGatewayClient.send_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1036\u001B[0m connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_connection()\n\u001B[1;32m   1037\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1038\u001B[0m     response \u001B[38;5;241m=\u001B[39m connection\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m   1039\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m binary:\n\u001B[1;32m   1040\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_connection_guard(connection)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py:511\u001B[0m, in \u001B[0;36mClientServerConnection.send_command\u001B[0;34m(self, command)\u001B[0m\n\u001B[1;32m    509\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    510\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 511\u001B[0m         answer \u001B[38;5;241m=\u001B[39m smart_decode(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream\u001B[38;5;241m.\u001B[39mreadline()[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m    512\u001B[0m         logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAnswer received: \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(answer))\n\u001B[1;32m    513\u001B[0m         \u001B[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001B[39;00m\n\u001B[1;32m    514\u001B[0m         \u001B[38;5;66;03m# answer before the socket raises an error.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/socket.py:705\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    703\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    704\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 705\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sock\u001B[38;5;241m.\u001B[39mrecv_into(b)\n\u001B[1;32m    706\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    707\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we stop the query to check the predictions",
   "id": "c108babd9497affc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T01:47:36.112876Z",
     "start_time": "2024-12-07T01:47:36.066797Z"
    }
   },
   "cell_type": "code",
   "source": "query.stop()",
   "id": "c2960eaf11e24fd4",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Node Failure Simulations\n",
    "### Test 1 - Do not crash\n",
    "Prediction results show 103 records\n",
    "\n",
    "### Test 2 - Crashed the node at 7 records\n",
    "Prediction results show 7 records\n",
    "\n",
    "### Test 3 - Crashed the node at 0 records\n",
    "Prediction results show 0 records\n",
    "This test is the most recent output"
   ],
   "id": "802b9ac1bb077ba7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "predictions = prediction_records_holder[0]\n",
    "print(f\"Number of predictions: {predictions.count()}\")\n",
    "predictions.show()"
   ],
   "id": "410e8614104ba906"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluating the model",
   "id": "6910c852f43723fe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Converting the columns _Class_\n",
    "Since we streamed our data from Kafka, the column _Class_ is of type StringType. We need to convert it to IntegerType in order to evaluate the model"
   ],
   "id": "51a43fd5f624743d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "predictions_updated = predictions.withColumn(\"Class\", col(\"Class\").cast(IntegerType()))\n",
    "predictions_updated.printSchema()"
   ],
   "id": "8573a1f485699600",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"Class\")\n",
    "print(f\"Area under ROC: {evaluator.evaluate(predictions_updated)}\")\n",
    "logger.info(f\"Area under ROC: {evaluator.evaluate(predictions_updated)}\")"
   ],
   "id": "29053c83a79312ac",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
